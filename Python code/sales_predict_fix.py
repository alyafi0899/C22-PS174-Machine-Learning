# -*- coding: utf-8 -*-
"""Salinan baru dari bangkit_predict2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sAgS_Pv4bfMM0NLBAi73ehejlJ4QStH9
"""

import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # visuals 

from sklearn.preprocessing import MinMaxScaler # scale the data
from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator # time series Generator
from tensorflow.keras.models import Sequential # Sequential model
from tensorflow.keras.layers import Dense, LSTM, Dropout # LSTM
from tensorflow.keras.callbacks import EarlyStopping # Early Stopping
import tensorflow as tf
# load the dataset
df = pd.read_csv('sales_forcasting.csv', parse_dates = True, index_col='DATE')
dataset = df.values
dataset = dataset.astype('float32')

df.head()

df.plot(subplots=True)

df.head()

# scaler 
# rename the column
df.columns = ['Sales']
# plot the data 
df.plot(figsize = (15,6));

# rename the column
df.columns = ['Sales']

# train test split 
test_size = 18
test_index = len(df) - test_size 

train = df.iloc[:test_index]
test = df.iloc[test_index:]

train.tail()

test.head()

# scaler 
scaler = MinMaxScaler()

# fit the scaler on the training data 
scaler.fit(train)

# use the scaler to transform training and test data 
scaled_train = scaler.transform(train)
scaled_test = scaler.transform(test)

# Timeseries Generator
length = 12 # a whole year 
train_generator = TimeseriesGenerator(scaled_train, scaled_train, length = length, batch_size = 1)
validation_generator = TimeseriesGenerator(scaled_test, scaled_test, length = length, batch_size = 1)

X, y = train_generator[0]

X

y

# number of features in our dataset
n_features = 1 

# build the model
model = Sequential()
model.add(LSTM(64, activation = 'relu', input_shape = (length, n_features)))
model.add(Dense(1))

# compile the model
model.compile(optimizer='adam', loss = 'mse')

# Early Stopping
early_stop = EarlyStopping(monitor='val_loss', patience=2)

# fir the model
history = model.fit(train_generator, epochs = 20, validation_data= validation_generator);

# plot losses 
losses = pd.DataFrame(model.history.history)
losses.plot(figsize = (12,6))

test_predictions = []

first_eval_batch = scaled_train[-length:]
current_batch = first_eval_batch.reshape((1, length, n_features))

for i in range(len(test)):
    
    # get prediction 1 time stamp ahead ([0] is for grabbing just the number instead of [array])
    current_pred = model.predict(current_batch)[0]
    
    # store prediction
    test_predictions.append(current_pred) 
    
    # update batch to now include prediction and drop first value
    current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1)

true_predictions = scaler.inverse_transform(test_predictions)
test['Predictions'] = true_predictions

test

test.plot(figsize = (12, 6));

# scale the full data 
full_scaler = MinMaxScaler()
scaled_full_data = full_scaler.fit_transform(df)

# generator for the full data 
length = 12
generator = TimeseriesGenerator(scaled_full_data, scaled_full_data, length = length, batch_size = 1)

# number of features in our dataset
n_features = 1 

# build the model
model = Sequential()
model.add(LSTM(64, activation = 'relu', input_shape = (length, n_features)))
model.add(Dense(1))

# compile the model
model.compile(optimizer='adam', loss = 'mse')

# fir the model
history = model.fit(generator, epochs = 10);

forecast = []
# Replace periods with whatever forecast length you want
periods = 12

first_eval_batch = scaled_full_data[-length:]
current_batch = first_eval_batch.reshape((1, length, n_features))

for i in range(periods):
    
    # get prediction 1 time stamp ahead ([0] is for grabbing just the number instead of [array])
    current_pred = model.predict(current_batch)[0]
    
    # store prediction
    forecast.append(current_pred) 
    
    # update batch to now include prediction and drop first value
    current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1)

# true values of the forcast  
forecast = scaler.inverse_transform(forecast)

# create a date index 
forecast_index = pd.date_range(start='2019-11-01',periods=periods,freq='MS')

# concatinate index with forcasts  
forecast_df = pd.DataFrame(data=forecast,index=forecast_index,
                           columns=['Forecast'])
forecast_df

ax = df.plot(figsize = (12,6))
forecast_df.plot(ax = ax)

# zoom in
ax = df.plot(figsize = (12,6))
forecast_df.plot(ax=ax)
plt.xlim('2018-01-01','2020-12-01')

"""# **Convert TFLite**"""

export_dir = 'saved_model'

# Use the tf.saved_model API to save your model in the SavedModel format
tf.saved_model.save(model, export_dir = export_dir)

# Select mode of optimization
mode = "Speed" 

if mode == 'Storage':
    optimization = tf.lite.Optimize.OPTIMIZE_FOR_SIZE
elif mode == 'Speed':
    optimization = tf.lite.Optimize.OPTIMIZE_FOR_LATENCY
else:
    optimization = tf.lite.Optimize.DEFAULT

import tensorflow as tf

converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)
converter.target_spec.supported_ops = [
  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.
  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.
]
tflite_model = converter.convert()
open("converted_model.tflite", "wb").write(tflite_model)

"""# **H5**"""

model.save('sales forcasting','/model_save_path')
model.save('trained_model.h5')
print(' Model saved ')

"""# **History.pkl**"""

def download_history():
  import pickle
  from google.colab import files

  with open('history.pkl', 'wb') as f:
    pickle.dump(history.history, f)

  files.download('history.pkl')

download_history()